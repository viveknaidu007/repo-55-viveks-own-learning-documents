{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "syntax for cnn , ann , rnn  in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANN:\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ANN, self).__init__()\n",
    "        # Define fully connected layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        # Define activation function\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass through the first fully connected layer\n",
    "        x = self.activation(self.fc1(x))\n",
    "        # Forward pass through the second fully connected layer\n",
    "        x = self.activation(self.fc2(x))\n",
    "        # Forward pass through the third fully connected layer\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define input size, hidden size, and output size\n",
    "input_size = 784  # Input size (assuming flattened images of 28x28 pixels)\n",
    "hidden_size = 256  # Hidden size of the ANN\n",
    "output_size = 10  # Output size (assuming 10 classes for classification)\n",
    "\n",
    "# Instantiate the ANN model\n",
    "model_ann = ANN(input_size, hidden_size, output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN:\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Define convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        # Define pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        # Define fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)  # Assuming 10 classes for classification\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass through convolutional layers\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv3(x)))\n",
    "        # Flatten the output for fully connected layers\n",
    "        x = x.view(-1, 64 * 4 * 4)\n",
    "        # Forward pass through fully connected layers\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the CNN model\n",
    "model_cnn = CNN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN:\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # Define the RNN layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        # Define the fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
    "        # Forward pass through the RNN layer\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        # Use the last hidden state for classification\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Define input size, hidden size, and output size\n",
    "input_size = 28  # Input size for each time step\n",
    "hidden_size = 128  # Hidden size of the RNN\n",
    "output_size = 10  # Output size (assuming 10 classes for classification)\n",
    "\n",
    "# Instantiate the RNN model\n",
    "model_rnn = RNN(input_size, hidden_size, output_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep Belief Networks (DBNs):\n",
    "\n",
    "\n",
    "#PyTorch doesn't have native support for DBNs, but you can implement them using Restricted Boltzmann Machines (RBMs) and stack them to create a DBN. Below is a basic example:\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RBM(nn.Module):\n",
    "    def __init__(self, visible_size, hidden_size):\n",
    "        super(RBM, self).__init__()\n",
    "        self.W = nn.Parameter(torch.randn(visible_size, hidden_size))\n",
    "        self.v_bias = nn.Parameter(torch.randn(visible_size))\n",
    "        self.h_bias = nn.Parameter(torch.randn(hidden_size))\n",
    "        \n",
    "    def forward(self, v):\n",
    "        h_prob = torch.sigmoid(torch.matmul(v, self.W) + self.h_bias)\n",
    "        h_sample = torch.bernoulli(h_prob)\n",
    "        v_prob = torch.sigmoid(torch.matmul(h_sample, self.W.t()) + self.v_bias)\n",
    "        return v_prob, h_prob\n",
    "\n",
    "# Create a DBN by stacking RBMs\n",
    "class DBN(nn.Module):\n",
    "    def __init__(self, rbm_layers):\n",
    "        super(DBN, self).__init__()\n",
    "        self.rbm_layers = nn.ModuleList(rbm_layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for rbm in self.rbm_layers:\n",
    "            x, _ = rbm(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "visible_size = 784  # Input size (assuming flattened images of 28x28 pixels)\n",
    "hidden_size = 256  # Hidden size of RBM layers\n",
    "rbm1 = RBM(visible_size, hidden_size)\n",
    "rbm2 = RBM(hidden_size, hidden_size)\n",
    "dbn = DBN([rbm1, rbm2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generative Adversarial Networks (GANs):\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_size),\n",
    "            nn.Tanh())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Example usage\n",
    "latent_size = 100  # Size of the input noise vector\n",
    "output_size = 784  # Output size (assuming flattened images of 28x28 pixels)\n",
    "input_size = output_size  # Size of discriminator input\n",
    "generator = Generator(latent_size, output_size)\n",
    "discriminator = Discriminator(input_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#autoencoders:\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU())\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_size, input_size),\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Example usage\n",
    "input_size = 784  # Input size (assuming flattened images of 28x28 pixels)\n",
    "hidden_size = 256  # Hidden size of the autoencoder\n",
    "model_autoencoder = Autoencoder(input_size, hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
